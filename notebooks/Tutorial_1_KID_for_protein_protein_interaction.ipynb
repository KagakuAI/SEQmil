{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1c02ba-af7b-4f92-a7a6-1885537e776d",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "### Synthetic Protein-Protein Interaction Dataset for Multi-Instance Learning (MIL) with Key Instance Detection (KID)\n",
    "\n",
    "Understanding protein-protein interactions (PPIs) is a central problem in biology, as these interactions drive most cellular processes. Experimentally characterizing all PPIs is costly and time-consuming, so **computational approaches** are increasingly used to predict and analyze interactions.\n",
    "\n",
    "### Why Synthetic Data?\n",
    "\n",
    "- Real PPI datasets can be limited, noisy, or biased.  \n",
    "- Synthetic datasets allow **controlled experiments**, where we know exactly which subsequences (motifs) are responsible for interactions.  \n",
    "- They are particularly useful for developing and benchmarking machine learning models.\n",
    "\n",
    "### Multi-Instance Learning (MIL)\n",
    "\n",
    "- In MIL, each data point is a **bag** containing multiple **instances**.  \n",
    "- A **bag is labeled positive** if it contains at least one \"key instance\" that triggers the label; otherwise, it is negative.  \n",
    "- Applied to PPIs:  \n",
    "  - Each bag = a pair of proteins.  \n",
    "  - Each instance = a pair of subsequences from the two proteins.  \n",
    "  - Positive bags contain subsequence pairs with specific **motifs** responsible for the interaction.  \n",
    "\n",
    "### Key Instance Detection (KID)\n",
    "\n",
    "- KID focuses on identifying **which instances in a positive bag are truly responsible** for the label.  \n",
    "- In PPI context: find the subsequence pairs (motifs) that drive the interaction.  \n",
    "- **KID accuracy** measures how well a model can detect these key instances, beyond just predicting the bag label.\n",
    "\n",
    "### Why This Setup is Useful\n",
    "\n",
    "- Enables **benchmarking MIL models** in a controlled setting.  \n",
    "- Provides **ground-truth key instances**, allowing detailed evaluation of model interpretability.  \n",
    "- Supports **feature engineering** (e.g., one-hot encoding of subsequences) and flexible model design.  \n",
    "\n",
    "With this setup, you can experiment with MIL algorithms, visualize motif contributions, and evaluate models both on bag-level predictions and key instance detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f02c00-04cd-4c05-8e59-1e1d73f417a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_bag_predictions(bag, predictions, top_k=10):\n",
    "    \"\"\"\n",
    "    Visualize a single MIL bag with predicted instance weights.\n",
    "    \n",
    "    Parameters:\n",
    "    - bag: dict with keys:\n",
    "        'protein_1' (str), 'protein_2' (str),\n",
    "        'subseqs_1' (list of str), 'subseqs_2' (list of str),\n",
    "        'key_indices' (list of int),\n",
    "        'label' (int),\n",
    "        'motifs' (tuple of str) or None\n",
    "    - predictions: 1D np.array or list of predicted weights, same length as number of instances in bag.\n",
    "    - top_k: int, number of top subsequence pairs to display\n",
    "    \n",
    "    Displays:\n",
    "    - Proteins with highlighted motif occurrences (in blue).\n",
    "    - Table of top_k pairs sorted by predicted weight, indicating true key instances.\n",
    "    \"\"\"\n",
    "    \n",
    "    p1 = bag['protein_1']\n",
    "    p2 = bag['protein_2']\n",
    "    subseqs_1 = bag['subseqs_1']\n",
    "    subseqs_2 = bag['subseqs_2']\n",
    "    key_indices = set(bag['key_indices'])\n",
    "    label = bag['label']\n",
    "    motifs = bag.get('motifs', None)  # now a tuple or None\n",
    "    \n",
    "    motif1, motif2 = (motifs if motifs is not None else (None, None))\n",
    "    \n",
    "    # Lengths\n",
    "    n_subseqs_1 = len(subseqs_1)\n",
    "    n_subseqs_2 = len(subseqs_2)\n",
    "    \n",
    "    # Find motif occurrences (positions) in each protein\n",
    "    def find_motif_positions(protein, motif):\n",
    "        positions = []\n",
    "        if motif:\n",
    "            start = 0\n",
    "            while True:\n",
    "                idx = protein.find(motif, start)\n",
    "                if idx == -1:\n",
    "                    break\n",
    "                positions.extend(range(idx, idx + len(motif)))\n",
    "                start = idx + 1\n",
    "        return set(positions)\n",
    "    \n",
    "    motif_positions_1 = find_motif_positions(p1, motif1)\n",
    "    motif_positions_2 = find_motif_positions(p2, motif2)\n",
    "    \n",
    "    # Visualize proteins with highlights for motif positions\n",
    "    def highlight_protein(seq, motif_pos_set):\n",
    "        \"\"\"Return a list of tuples (char, highlight_bool)\"\"\"\n",
    "        highlight_mask = [i in motif_pos_set for i in range(len(seq))]\n",
    "        return [(c, h) for c, h in zip(seq, highlight_mask)]\n",
    "    \n",
    "    h_protein_1 = highlight_protein(p1, motif_positions_1)\n",
    "    h_protein_2 = highlight_protein(p2, motif_positions_2)\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 1, figsize=(14, 8))\n",
    "    fig.suptitle(f\"Bag label: {label} - Key instances: {len(key_indices)}\", fontsize=16)\n",
    "    \n",
    "    def plot_highlighted_sequence(ax, highlighted_seq, title):\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "        for i, (char, highlight) in enumerate(highlighted_seq):\n",
    "            color = 'blue' if highlight else 'black'\n",
    "            bbox = dict(facecolor='lightblue', alpha=0.5) if highlight else None\n",
    "            ax.text(i, 0, char, color=color, fontsize=14, fontweight='bold' if highlight else 'normal', bbox=bbox)\n",
    "        ax.set_xlim(-1, len(highlighted_seq))\n",
    "        ax.set_ylim(-1, 1)\n",
    "    \n",
    "    plot_highlighted_sequence(axs[0], h_protein_1, f\"Protein 1 (motif: {motif1 if motif1 else 'None'})\")\n",
    "    plot_highlighted_sequence(axs[1], h_protein_2, f\"Protein 2 (motif: {motif2 if motif2 else 'None'})\")\n",
    "    \n",
    "    # Sort predicted weights descending and get top_k pairs\n",
    "    pred = np.array(predictions)\n",
    "    top_indices = np.argsort(pred)[::-1][:top_k]\n",
    "    \n",
    "    # Prepare pairs info for display\n",
    "    rows = []\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        i = idx // n_subseqs_2\n",
    "        j = idx % n_subseqs_2\n",
    "        s1 = subseqs_1[i]\n",
    "        s2 = subseqs_2[j]\n",
    "        weight = pred[idx]\n",
    "        is_key = \"YES\" if idx in key_indices else \"NO\"\n",
    "        rows.append((rank, i, s1, j, s2, f\"{weight:.4f}\", is_key))\n",
    "    \n",
    "    # Display table below sequences\n",
    "    col_labels = [\"Rank\", \"Subseq idx P1\", \"Subseq P1\", \"Subseq idx P2\", \"Subseq P2\", \"Predicted weight\", \"Is Key Instance\"]\n",
    "    table_data = [[str(x) for x in row] for row in rows]\n",
    "    \n",
    "    axs[2].axis('off')\n",
    "    table = axs[2].table(cellText=table_data, colLabels=col_labels, loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4e30d9-ed7a-4362-9025-2b658521027d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mERROR)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "from milearn.preprocessing import BagMinMaxScaler\n",
    "\n",
    "# Network hparams\n",
    "from milearn.network.module.hopt import DEFAULT_PARAM_GRID\n",
    "\n",
    "# MIL wrappers\n",
    "from milearn.network.regressor import BagWrapperMLPNetworkRegressor, InstanceWrapperMLPNetworkRegressor\n",
    "from milearn.network.classifier import BagWrapperMLPNetworkClassifier, InstanceWrapperMLPNetworkClassifier\n",
    "\n",
    "# MIL networks\n",
    "from milearn.network.classifier import (InstanceNetworkClassifier,\n",
    "                                        BagNetworkClassifier,\n",
    "                                        AdditiveAttentionNetworkClassifier,\n",
    "                                        SelfAttentionNetworkClassifier,\n",
    "                                        HopfieldAttentionNetworkClassifier,\n",
    "                                        DynamicPoolingNetworkClassifier)\n",
    "\n",
    "# Utils\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data\n",
    "from seqmil.data import create_ppi_dataset\n",
    "from seqmil.featurizer import OneHotFeaturizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c83f5-303e-4b42-b67d-f4327830cc02",
   "metadata": {},
   "source": [
    "### 1. Create synthetic PPI dataset\n",
    "\n",
    "This dataset simulates protein-protein interactions for **multi-instance learning (MIL)**, where each *bag* represents a protein pair with multiple subsequence pairs (instances). Some instances are **key instances** containing specific motifs that define positive interactions.\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "Each **bag** corresponds to a pair of proteins (`protein_1` and `protein_2`).  \n",
    "\n",
    "- **Positive bags**: Contain specific motif pairs, i.e., `motif1` in `protein_1` and `motif2` in `protein_2`.  \n",
    "- **Negative bags**: Do **not** contain any of the specified motifs.  \n",
    "\n",
    "Each protein is split into overlapping subsequences using a sliding window. The **bag instances** are all possible pairs of subsequences between the two proteins.\n",
    "\n",
    "### Step-by-Step Dataset Creation\n",
    "\n",
    "**Parameter Setup**\n",
    "   - `n_bags`: Number of bags to generate  \n",
    "   - `pos_ratio`: Fraction of positive bags  \n",
    "   - `protein_length`: Length of synthetic protein sequences  \n",
    "   - `window_size` & `step_size`: Sliding window parameters for subsequences  \n",
    "   - `motif_pairs`: List of motif tuples used to generate positive bags\n",
    "\n",
    "**Protein Sequence Generation.**\n",
    "   - Each protein is randomly generated from 20 standard amino acids (`ACDEFGHIKLMNPQRSTVWY`).  \n",
    "\n",
    "**Motif Embedding (for positive bags)**\n",
    "   - A motif pair `(motif1, motif2)` is randomly selected  \n",
    "   - `motif1` is embedded in `protein_1` and `motif2` in `protein_2` at random positions  \n",
    "\n",
    "**Sliding Window Extraction**\n",
    "   - Each protein is split into overlapping subsequences of length `window_size`, moving by `step_size`  \n",
    "   - Example: For a protein of length 50, `window_size=10`, `step_size=3`, we get subsequences like `protein[0:10]`, `protein[3:13]`, `protein[6:16]`, etc.  \n",
    "\n",
    "**Bag Construction**\n",
    "   - Every subsequence of `protein_1` is paired with every subsequence of `protein_2`  \n",
    "   - These pairs form the **instances** of the bag  \n",
    "   - **Key instances** are those where both motifs are present in the corresponding subsequences  \n",
    "\n",
    "**Negative Bag Filtering**\n",
    "   - Negative bags are generated by random proteins that **do not contain any motifs** from `motif_pairs`  \n",
    "\n",
    "**Featurization (optional)**\n",
    "   - Subsequence pairs are converted to vectors using a featurizer, e.g., `OneHotFeaturizer`  \n",
    "   - Each subsequence is one-hot encoded and concatenated to represent the instance vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397eab5d-a247-4938-ac0b-74819aea3bca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "motif_pairs = [\n",
    "    # (\"ALRMNDU\", \"APRQKVN\"), \n",
    "    (\"KLMNPQR\", \"STVWYAC\")]\n",
    "\n",
    "dataset = create_ppi_dataset(\n",
    "    motif_pairs=motif_pairs,\n",
    "    n_bags=1000,\n",
    "    pos_ratio=0.5,\n",
    "    protein_length=50,\n",
    "    window_size=10,\n",
    "    step_size=3,\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f06de-2169-4a9c-8e3d-3b3d4c463f9e",
   "metadata": {},
   "source": [
    "### 2. Encode protein sequences \n",
    "\n",
    "After generating the raw dataset, each subsequence pair in a bag needs to be converted into a numerical vector for machine learning.  \n",
    "\n",
    "**Process:**\n",
    "\n",
    "**One-Hot Encoding**  \n",
    "   - Each amino acid in a subsequence is represented as a one-hot vector of length 20 (one for each standard amino acid).  \n",
    "   - Example: `'A'` → `[1,0,0,...,0]`, `'C'` → `[0,1,0,...,0]`.  \n",
    "\n",
    "**Subsequence Pair Encoding**  \n",
    "   - For each instance `(s1, s2)` in a bag, both subsequences are one-hot encoded separately.  \n",
    "   - The vectors are then concatenated to form a single vector representing the pair.  \n",
    "\n",
    "**Bag Representation**  \n",
    "   - All concatenated instance vectors in a bag are stacked into a NumPy array: `encoded_bag`.  \n",
    "   - This array can then be used as input for MIL models, while `key_indices` indicate the key instances in positive bags.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3962e85-6d19-4632-8d65-a701f4f2de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = OneHotFeaturizer()\n",
    "\n",
    "for bag in dataset:\n",
    "    encoded_bag = []\n",
    "    for s1, s2 in bag[\"raw_bag\"]:\n",
    "        v1, v2 = featurizer.run([s1, s2])\n",
    "        pair_vec = np.concatenate([v1, v2])\n",
    "        encoded_bag.append(pair_vec)\n",
    "    bag[\"encoded_bag\"] = np.stack(encoded_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a53cc8-013c-4fce-a3bf-28def229249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['encoded_bag'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9788b8f-1de7-4443-a4b6-0d30247071a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['motifs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288da55-c2d8-4e02-927d-93ae24b00e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]['protein_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b4ed7-8292-4fbe-a5ce-f27f69c45172",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]['protein_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bae9a2-acb3-41a9-8da1-24b55b8ac486",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1][\"key_indices\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d06bc6-f9cf-4a7c-833c-6032d5638ee0",
   "metadata": {},
   "source": [
    "### Key Instance Detection (KID) Accuracy\n",
    "\n",
    "In multi-instance learning, some instances in a bag are more important—they are the **key instances** that determine whether the bag is positive.  \n",
    "\n",
    "The KID accuracy measures **how well a model can identify these key instances** in positive bags.  \n",
    "\n",
    "- A bag is counted as correct if the model’s top predictions include at least one of the true key instances.  \n",
    "- The final KID accuracy is the fraction of positive bags where the key instance was successfully detected.  \n",
    "\n",
    "This metric focuses on whether the model can find the most important parts of a bag, rather than just predicting the bag’s overall label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8536c4-224d-47b2-8058-79d30b72dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== KID Accuracy Function ====\n",
    "def kid_accuracy(true_key_indices, predictions, labels, top_k=1):\n",
    "    assert len(predictions) == len(true_key_indices) == len(labels)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for pred, true_keys, label in zip(predictions, true_key_indices, labels):\n",
    "        if label != 1:\n",
    "            continue\n",
    "        total += 1\n",
    "\n",
    "        if len(true_keys) == 0:\n",
    "            continue\n",
    "\n",
    "        top_pred_indices = np.argsort(pred)[-top_k:][::-1]  # ensure descending order\n",
    "        if any(idx in true_keys for idx in top_pred_indices):\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28cc663-80ef-4629-be79-26612251e293",
   "metadata": {},
   "source": [
    "### 3. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408368f2-94e9-4220-8517-8158e650181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract encoded bags, labels, key indices, and raw sequences\n",
    "raw_bags = [bag[\"raw_bag\"] for bag in dataset]\n",
    "encoded_bags = [bag[\"encoded_bag\"] for bag in dataset]\n",
    "labels = [bag[\"label\"] for bag in dataset]\n",
    "key_indices = [bag[\"key_indices\"] for bag in dataset]\n",
    "\n",
    "\n",
    "\n",
    "# Train/test split\n",
    "x_train, x_test, y_train, y_test, key_train, key_test, seq_train, seq_test = train_test_split(\n",
    "    encoded_bags, labels, key_indices, dataset,\n",
    "    test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Scaling (using BagMinMaxScaler or any bag-level scaler)\n",
    "scaler = BagMinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03caf00d-2437-4178-864c-1f1ea02d18ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = DynamicPoolingNetworkClassifier()\n",
    "# model.hopt(x_train_scaled, y_train, param_grid=DEFAULT_PARAM_GRID, verbose=True)\n",
    "model.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead5cc0-846e-4fcc-99ad-9c61c4af8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict(x_test_scaled)\n",
    "y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "\n",
    "w_pred = model.get_instance_weights(x_test_scaled)\n",
    "w_pred = [w.flatten() for w in w_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d343140-e14a-4f6d-84fa-7fcf6f8e9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classification accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"KID accuracy: {kid_accuracy(key_test, w_pred, y_test, top_k=1):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ac508-6f81-4cba-af61-f6b6dd6cd552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bags with correct positive class prediction (use these indexes for visualization)\n",
    "np.where((y_pred == 1) & (np.array(y_test) == 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3872a1-10be-49b5-8673-ddd1fd474782",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "visualize_bag_predictions(seq_test[N], w_pred[N], top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9a65b-1d30-4746-b99c-c0480486526c",
   "metadata": {},
   "source": [
    "### 4. Mini-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf5d47-50d5-4fb0-a775-ee0c2c132f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_list = [\n",
    "    \n",
    "        # attention mil networks\n",
    "        (\"AdditiveAttentionNetworkClassifier\", AdditiveAttentionNetworkClassifier()),\n",
    "        (\"SelfAttentionNetworkClassifier\", SelfAttentionNetworkClassifier()),\n",
    "        (\"HopfieldAttentionNetworkClassifier\", HopfieldAttentionNetworkClassifier()),\n",
    "\n",
    "        # other mil networks\n",
    "        (\"DynamicPoolingNetworkClassifier\", DynamicPoolingNetworkClassifier()),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe494fa4-4eda-4444-ac2b-5f38bd4bef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame()\n",
    "for model_idx, (name, model) in enumerate(classifier_list, 1):\n",
    "    print(f\"  [Model {model_idx}/{len(classifier_list)}] Training model: '{name}'\")\n",
    "\n",
    "    # train model\n",
    "    # model.hopt(x_train_scaled, y_train, param_grid=DEFAULT_PARAM_GRID, verbose=True)\n",
    "    model.fit(x_train_scaled, y_train)\n",
    "    # predict\n",
    "    y_pred = model.predict(x_test_scaled)\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    w_pred = model.get_instance_weights(x_test_scaled)\n",
    "    w_pred = [w.flatten() for w in w_pred]\n",
    "    #\n",
    "    res_df.loc[name, \"PRED_ACC\"] = accuracy_score(y_test, y_pred)\n",
    "    res_df.loc[name, \"KID_RANK\"] = kid_accuracy(key_test, w_pred, y_test, top_k=1)\n",
    "\n",
    "print(\"\\nAll models completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da8bcd-a6cd-4177-b02f-5c12b7219884",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac8c7a-d124-429d-92b0-1ee64f249b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63837ea9-8efe-4105-9255-127c8d5b29ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seqmil",
   "language": "python",
   "name": "seqmil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
